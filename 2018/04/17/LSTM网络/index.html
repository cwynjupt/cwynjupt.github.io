<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <link rel="stylesheet" href="/style/style.css">
<script>
    var NlviConfig = {
        title: "努力努力再努力",
        author: "陈文燕",
        theme: "banderole",
        lightbox: true,
        animate: true,
        baseUrl: "/",
        search: true
    }
</script>



    <link rel="stylesheet" href="/script/lib/lightbox/css/lightbox.min.css">




    <link rel="stylesheet" href="/syuanpi/syuanpi.min.css">










<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="browsermode" content="application">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-title" content="努力努力再努力">
<meta name="apple-mobile-web-app-status-bar-style" content="default">
<meta name="msapplication-navbutton-color" content="#666666">
<meta name= "format-detection" content="telephone=no" />
<meta name="keywords" content="nlvi, Nlvi" />


  <meta name="subtitle" content="努力努力再努力">


  <meta name="description" content="cwy's blog">



  <title> LSTM网络 · 努力努力再努力 </title>
</head>
<body>
  <div class="progress">
  <div class="progress-inner"></div>
</div>
  
  
    <div class="tagcloud-mask"></div>  
<div class="tagcloud" id="tagcloud">
  <div class="tagcloud-inner">
    
  </div>
</div>  
  

  <div class="container">

    <header class="header" id="header">
  <div class="header-wrapper">
    <div class="logo">
  <div class="logo-inner syuanpi tvIn">
    <span><a href="/">努力努力再努力</a></span>
    
      <span id="subtitle">努力努力再努力</span>
    
  </div>
</div>
    <nav class="main-nav">
        
  <ul class="main-nav-list syuanpi tvIn">
  
    <li class="menu-item">
      <a href="javascript:;" id="search">
        <span>搜索</span>
        
          <span class="menu-item-label">search</span>
        
      </a>
    </li>
  
  
    
      
    
    <li class="menu-item">
      <a href="/" id="article">
        <span class="base-name">文章</span>
        
          <span class="menu-item-label">article</span>
        
      </a>
    </li>  
  
    
      
    
    <li class="menu-item">
      <a href="/archives" id="archives">
        <span class="base-name">归档</span>
        
          <span class="menu-item-label">archives</span>
        
      </a>
    </li>  
  
    
      
    
    <li class="menu-item">
      <a href="javascript:;" id="tags">
        <span class="base-name">标签</span>
        
          <span class="menu-item-label">tags</span>
        
      </a>
    </li>  
  
    
      
    
    <li class="menu-item">
      <a href="/about" id="about">
        <span class="base-name">关于</span>
        
          <span class="menu-item-label">about</span>
        
      </a>
    </li>  
  
  </ul>
  
</nav>
    
    
  </div>
</header>
<div class="mobile-header">
  <div class="mobile-header-body">
    <div class="mobile-header-list">
      
        
            <div class="mobile-nav-item">
                <a href="/">
                    <span>文章</span>
                    
                        <span class="menu-item-label">article</span>
                    
                </a>
            </div>
        
      
        
            <div class="mobile-nav-item">
                <a href="/archives">
                    <span>归档</span>
                    
                        <span class="menu-item-label">archives</span>
                    
                </a>
            </div>
        
      
        
          <div class="mobile-nav-item inner-cloud">
            <div class="mobile-nav-tag">
              <a href="javascript:;" id="mobile-tags">
                <span>标签</span>
                
                    <span class="menu-item-label">tags</span>
                
              </a>
            </div>
            <div class="mobile-nav-tagcloud">
              <div class="mobile-tagcloud-inner">
                
              </div>
            </div>
          </div>
        
      
        
            <div class="mobile-nav-item">
                <a href="/about">
                    <span>关于</span>
                    
                        <span class="menu-item-label">about</span>
                    
                </a>
            </div>
        
      
    </div>
  </div>
  <div class="mobile-header-nav">
    <div class="mobile-header-item" id="mobile-left">
      <div class="header-menu-item">
        <span class="header-menu-line"></span>
      </div>
    </div>
    <h1 class="mobile-header-title">
      <a href="/">努力努力再努力</a>
    </h1>
    <div class="mobile-header-item"></div>
  </div>
</div>
    <div class="container-inner">
      <main class="main" id="main">
        <div class="main-wrapper">
          
    
  <article class="
  post
   is_post 
  ">
    <header class="post-header">
      <div class="post-time syuanpi riseIn-light back-1">
        <div class="post-time-wrapper">
          <span>2018-04-17</span>
          
          
            
          
        </div>
      </div>
      <h1 class="post-title syuanpi riseIn-light back-2">
        
          LSTM网络
        
      </h1>
    </header>
    <div class="post-content syuanpi riseIn-light back-3">
      
        <h1 id="LSTM-网络"><a href="#LSTM-网络" class="headerlink" title="LSTM 网络"></a>LSTM 网络</h1><p>LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，而非需要付出很大代价才能获得的能力！<a id="more"></a></p>
<p>所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/42741-9ac355076444b66f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>标准 RNN 中的重复模块包含单一的层</p>
<p>LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于 单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。</p>
<p>LSTM的特点就是在RNN结构以外添加了各层的阀门节点。阀门有3类：<strong>遗忘阀门（forget gate）</strong> ，<strong>输入阀门（input gate）</strong> 和<strong>输出阀门（output gate）</strong> 。这些阀门可以打开或关闭，用于将判断模型网络的记忆态（之前网络的状态）在该层输出的结果是否达到阈值从而加入到当前该层的计算中。如图中所示，阀门节点利用sigmoid函数将网络的记忆态作为输入计算；如果输出结果达到阈值则将该阀门输出与当前层的的计算结果相乘作为下一层的输入（<strong>PS：这里的相乘是在指矩阵中的逐元素相乘</strong>）；如果没有达到阈值则将该输出结果遗忘掉。每一层包括阀门节点的权重都会在每一次模型反向传播训练过程中更新。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/42741-b9a16a53d58ca2b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>不必担心这里的细节。我们会一步一步地剖析 LSTM 解析图。现在，我们先来熟悉一下图中使用的各种元素的图标。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/42741-ea943b818b8e18d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>LSTM 中的图标</p>
<p>在上面的图例中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表 pointwise 的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。</p>
<h2 id="LSTM-的核心思想"><a href="#LSTM-的核心思想" class="headerlink" title="LSTM 的核心思想"></a>LSTM 的核心思想</h2><p>LSTM 的关键就是细胞状态，水平线在图上方贯穿运行。</p>
<p>细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/42741-ac1eb618f37a9dea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>LSTM 有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/42741-7169541c790efd13.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”！</p>
<p>LSTM 拥有三个门，来保护和控制细胞状态。</p>
<h2 id="逐步理解-LSTM"><a href="#逐步理解-LSTM" class="headerlink" title="逐步理解 LSTM"></a>逐步理解 LSTM</h2><p>在我们 LSTM 中的第一步是决定我们会从细胞状态中丢弃什么信息。这个决定通过一个称为<strong>忘记门层</strong>完成。该门会读取 $h_{t-1}$ 和 $x_t$，输出一个在 0 到 1 之间的数值给每个在细胞状态 $C_{t-1}$ 中的数字。1 表示“完全保留”，0 表示“完全舍弃”。</p>
<p>让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前<strong>主语</strong>的性别，因此正确的<strong>代词</strong>可以被选择出来。当我们看到新的<strong>主语</strong>，我们希望忘记旧的<strong>主语</strong>。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/42741-96b387f711d1d12c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>下一步是确定什么样的新信息被存放在细胞状态中。这里包含两个部分。第一，sigmoid 层称 “输入门层” 决定什么值我们将要更新。然后，一个 tanh 层创建一个新的候选值向量，$\tilde{C}_t$，会被加入到状态中。下一步，我们会讲这两个信息来产生对状态的更新。</p>
<p>在我们语言模型的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/42741-7fa07e640593f930.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>现在是更新旧细胞状态的时间了，$C_{t-1}$ 更新为 $C_t$。前面的步骤已经决定了将会做什么，我们现在就是实际去完成。</p>
<p>我们把旧状态与 $f_t$ 相乘，丢弃掉我们确定需要丢弃的信息。接着加上 $i_t * \tilde{C}_t$。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。</p>
<p>在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新的信息的地方。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/42741-d88caa3c4faf5353.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。</p>
<p>在语言模型的例子中，因为他就看到了一个 <strong>代词</strong>，可能需要输出与一个 <strong>动词</strong> 相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/42741-4c9186bf786063d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="LSTM-的变体"><a href="#LSTM-的变体" class="headerlink" title="LSTM 的变体"></a>LSTM 的变体</h2><p>我们到目前为止都还在介绍正常的 LSTM。但是不是所有的 LSTM 都长成一个样子的。实际上，几乎所有包含 LSTM 的论文都采用了微小的变体。差异非常小，但是也值得拿出来讲一下。</p>
<p>其中一个流形的 LSTM 变体，就是由 <a href="https://link.jianshu.com?t=ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank" rel="noopener">Gers &amp; Schmidhuber (2000)</a> 提出的，增加了 “peephole connection”。是说，我们让 门层 也会接受细胞状态的输入。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/42741-0f80ad5540ea27f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上面的图例中，我们增加了 peephole 到每个门上，但是许多论文会加入部分的 peephole 而非所有都加。</p>
<p>另一个变体是通过使用 coupled 忘记和输入门。不同于之前是分开确定什么忘记和需要添加什么新的信息，这里是一同做出决定。我们仅仅会当我们将要输入在当前位置时忘记。我们仅仅输入新的值到那些我们已经忘记旧的信息的那些状态 。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/42741-bd2f1feaea22630e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>另一个改动较大的变体是 Gated Recurrent Unit (GRU)，这是由 Cho, et al. (2014)提出。它将忘记门和输入门合成了一个单一的 更新门。同样还混合了细胞状态和隐藏状态，和其他一些改动。最终的模型比标准的 LSTM 模型要简单，也是非常流行的变体。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/42741-dd3d241fa44a71c0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里只是部分流行的 LSTM 变体。当然还有很多其他的，如Yao, et al. (2015) 提出的 Depth Gated RNN。还有用一些完全不同的观点来解决长期依赖的问题，如Koutnik, et al. (2014)提出的 Clockwork RNN。</p>
<p>要问哪个变体是最好的？其中的差异性真的重要吗？<a href="https://link.jianshu.com?t=http://arxiv.org/pdf/1503.04069.pdf" target="_blank" rel="noopener">Greff, et al. (2015)</a> 给出了流行变体的比较，结论是他们基本上是一样的。<a href="https://link.jianshu.com?t=http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" target="_blank" rel="noopener">Jozefowicz, et al. (2015)</a> 则在超过 1 万种 RNN 架构上进行了测试，发现一些架构在某些任务上也取得了比 LSTM 更好的结果。</p>
<p>以上是LSTM的前向计算过程，下面介绍求解梯度的反向传播算法。</p>
<h2 id="梯度求解：BPTT-随时间反向传播"><a href="#梯度求解：BPTT-随时间反向传播" class="headerlink" title="梯度求解：BPTT 随时间反向传播"></a>梯度求解：BPTT 随时间反向传播</h2><p>(1) 前向计算各时刻遗忘门状态 ft、输入门状态it、当前输入新信息 C¯t、细胞状态 Ct、输出门状态 ot、隐藏层状态 ht、模型输出 yt； </p>
<p>(2) 反向传播计算误差 δ，即模型目标函数 E 对加权输入 nett=(Wh∗ht−1+Wx∗xt+b)的偏导；(注意，δ 的传播沿两个方向，分别为从输出层传递至输入层，以及沿时间 t的反向传播) </p>
<p>(3) 求解模型目标函数 E对权重Whf,Wxf,bf;Whi,Wxi,bi;Whc,Wxc,bc;Who,Wxo,bo;Wy,by</p>
<p>的偏导数。<strong>δ</strong>沿时间 t <strong>的反向传播</strong> 定义 δht=∂E∂ht</p>
<p>由于 ht=ot∗tanh(Ct)</p>
<p>可得 δot=∂E∂ot=δht∗tanh(Ct)</p>
<p>δCt+=δht∗ot∗(1−tanh2(Ct))</p>
<p>注意，由于 Ct记忆了所有时刻的细胞状态，故每个时间点迭代时，δCt累加。</p>
<p>由于 Ct=it∗C¯t+ft∗Ct−1</p>
<p>则 δit=δCt∗C¯t</p>
<p>δft=δCt∗Ct−1</p>
<p>δCt¯=δCt∗it</p>
<p>δCt−1=δCt∗ft</p>
<p>由于 it=f(netit)=sigmoid(Whi∗ht−1+Wxi∗xt+bi)</p>
<p>ft=f(netft)=sigmoid(Whf∗ht−1+Wxf∗xt+bf)</p>
<p>C¯t=f(netC¯t)=tanh(WhC¯∗ht−1+WxC¯∗xt+bC¯)</p>
<p>it=f(netot)=sigmoid(Who∗ht−1+Wxo∗xt+bo)</p>
<p>故 </p>
<p>δnetit=δit∗f′(netit)=δit∗it∗(1−it)</p>
<p>δnetft=δft∗f′(netft)=δft∗ft∗(1−ft)</p>
<p>δnetC¯t=δC¯t∗f′(netC¯t)=δC¯t∗(1−C¯2t)</p>
<p>δnetot=δot∗f′(netot)=δot∗ot∗(1−ot)</p>
<p>最后，可求得各个权重矩阵的偏导数</p>
<p>∂E∂Whi+=δnetit∗hTt−1,∂E∂Wxi+=δnetit∗xTt,∂E∂bi+=δnetit</p>
<p>∂E∂Whf+=δnetft∗hTt−1,∂E∂Wxf+=δnetft∗xTt,∂E∂bf+=δnetft</p>
<p>∂E∂WhC¯+=δnetC¯t∗hTt−1,∂E∂WxC¯+=δnetC¯t∗xTt,∂E∂bC¯+=δnetC¯t</p>
<p>∂E∂Who+=δnetot∗hTt−1,∂E∂Wxo+=δnetot∗xTt,∂E∂bo+=δnetot</p>
<p>注意以上权重参数在所有时刻共享，故每个时间点迭代时梯度累加。</p>
<p><strong>某一时刻 t,δ</strong> <strong>从输出层传递至输入层</strong></p>
<p>对于输出层 L ：由于 yt=g(Wy∗ht+by)=g(nett)，则 δLnett=δEδyt∗g′(nett)可求得权重矩阵 Wy,by的偏导数 ：</p>
<p>∂E∂WY=δLnett∗hTt</p>
<p>∂E∂bY=δLnett</p>
<p>也可得 δLht=WTy∗δLnett</p>
<p> 对于其它层 l ： </p>
<p>由 nett=(Wh∗ht−1+Wx∗xt+b)</p>
<p>δl−1ht=δEδhl−1t=δlht∗δhltδhl−1t</p>
<p>因为 δlht∗δhltδhl−1t=δlht∗δhltδolt∗δoltδhl−1t+δlht∗δhltδclt∗δcltδiltδiltδhl−1t+δlht∗δhltδclt∗δcltδc¯ltδc¯ltδhl−1t+δlht∗δhltδclt∗δcltδfltδfltδhl−1t</p>
<p>故 δl−1ht=δlht∗δhltδhl−1t=δlotT∗Wxo∗f′(netlot)+δlitT∗Wxi∗f′(netlit)+δlc¯tT∗Wxc¯∗f′(netlc¯t)+δlftT∗Wxf∗f′(netlft)</p>
<p> 以上是LSTM各参数的梯度求解过程，下面依照以上公式，实现一个简单的单层LSTM网络。</p>
<p>参考：<a href="https://www.jianshu.com/p/9dc9f41f0b29" target="_blank" rel="noopener">https://www.jianshu.com/p/9dc9f41f0b29</a> <a href="https://blog.csdn.net/flyinglittlepig/article/details/72229041" target="_blank" rel="noopener">https://blog.csdn.net/flyinglittlepig/article/details/72229041</a></p>

      
    
    </div>
    
      
      
    
  </article>
  
    
  <nav class="article-page">
    
      <a href="/2018/04/17/RNN模型/" id="art-left" class="art-left">
        <span class="next-title">
          <i class="iconfont icon-left"></i>RNN模型
        </span>
      </a>
    
    
      <a href="/2018/04/13/PCA故障检测/" id="art-right" class="art-right">
        <span class="prev-title"> 
          PCA故障检测<i class="iconfont icon-right"></i>  
        </span>
      </a>
    
  </nav>

    
  <i id="com-switch" class="iconfont icon-more jumping-in long infinite" style="font-size:24px;display:block;text-align:center;transform:rotate(180deg);"></i>
  <div class="post-comments" id="post-comments" style="display: block;margin: auto 16px;">
    
    
    

  </div>


  
   
    
  
  <aside class="post-toc">
    <span class="title" id="toc-switch"><span>文章导航</span></span>
    <div class="toc-inner syuanpi back-1 fallIn-light">
      <li class="title-link"><a href="javascript:;" class="toTop">LSTM网络</a></li>
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#LSTM-网络"><span class="toc-text">LSTM 网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#LSTM-的核心思想"><span class="toc-text">LSTM 的核心思想</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#逐步理解-LSTM"><span class="toc-text">逐步理解 LSTM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LSTM-的变体"><span class="toc-text">LSTM 的变体</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度求解：BPTT-随时间反向传播"><span class="toc-text">梯度求解：BPTT 随时间反向传播</span></a></li></ol></li></ol>
    </div>
  </aside>


  


        </div>
      </main>

      <footer class="footer syuanpi fadeIn" id="footer">
  <hr>
  <div class="footer-wrapper">
    <div class="left">
      <div class="contact-icon">
    
    
    
    
    
    
    
    
        
        
        
        
        
            <a href="https://www.facebook.com/1" class="iconfont icon-facebook" title="facebook"></a>
        
        
        
    
        
        
        
            <a href="https://twitter.com/2" class="iconfont icon-twitter" title="twitter"></a>
        
        
        
        
        
    
        
        
        
        
        
        
        
            <a href="https://www.instagram.com/3" class="iconfont icon-ins" title="instagram"></a>
        
    
        
        
            <a href="https://weibo.com/4" class="iconfont icon-weibo" title="weibo"></a>
        
        
        
        
        
        
    
        
        
        
        
            <a href="https://www.zhihu.com/people/5" class="iconfont icon-zhihu" title="zhihu"></a>
        
        
        
        
    
        
            <a href="https://github.com/6" class="iconfont icon-github" title="github"></a>
        
        
        
        
        
        
        
    
        
        
        
        
        
        
            <a href="https://www.linkedin.com/in/7" class="iconfont icon-linkedin" title="linkedin"></a>
        
        
    
</div>
    </div>
    <div class="right">
      <div class="copyright">
    <div class="info">
        <span>&copy;</span>
        <span>2016 ~ 2018</span>
        <span>❤</span>
        <span>陈文燕</span>
    </div>
    <div class="theme">
        <span>
            动力来源于
            <a href="http://hexo.io/" target="_blank">Hexo </a>
        </span>
        <span>
            主题
            <a href="https://github.com/ColMugX/hexo-theme-Nlvi"> Nlvi </a>
        </span>
    </div>
    
</div>
    </div>
  </div>
</footer>
    </div>
  </div>
  <script src="/script/lib/jquery/jquery-3.2.1.min.js"></script>


    <script src="/script/lib/lightbox/js/lightbox.min.js"></script>






<script src="/script/src/nlvi.js"></script>
<script src="/script/src/utils.js"></script>
<script src="/script/scheme/balance.js"></script>
<script src="/script/src/plugins.js"></script>
<script src="/script/bootstarp.js"></script>


<div class="backtop syuanpi dead toTop" id="backtop">
    <i class="iconfont icon-up"></i>
    <span style="text-align:center;font-family:Georgia;"><span style="font-family:Georgia;" id="scrollpercent">1</span>%</span>
</div>


  <div class="search" id="search">
    <div class="mask" id="mask"></div>
    <div class="search-wrapper syuanpi">
      <h2 id="search-header" class="syuanpi">搜索一下？</h2>
      <div class="input">
        <input type="text" id="local-search-input" results="0" name="">
      </div>
      <div id="local-search-result"></div>
    </div>
  </div>
  <script>
  if (NlviConfig.theme === 'banderole') {
    var GREETING = {
      morning: "我的博客里可没有早饭！",
      noon: "记得午休吃饭，不着急看我的博客~",
      after: "下午好，感谢查阅我的博客",
      night: "您来我的博客看笑话当做休息嘛",
      midnight: "我觉得大半夜就不要看啦"
    }
  }
  </script>

</body>
</html>
